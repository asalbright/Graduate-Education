{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_gym_wrappers_saving_loading.ipynb","provenance":[{"file_id":"https://github.com/araffin/rl-tutorial-jnrr19/blob/sb3/2_gym_wrappers_saving_loading.ipynb","timestamp":1607980584407}],"collapsed_sections":["UEpTys28Wz05"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3ezJ3Y7XRUnj"},"source":["# Stable Baselines3 Tutorial - Gym wrappers, saving and loading models\n","\n","Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n","\n","Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n","\n","Documentation: https://stable-baselines3.readthedocs.io/en/master/\n","\n","RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n","\n","\n","## Introduction\n","\n","In this notebook, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n","\n","\n","You will also see the *loading* and *saving* functions, and how to read the outputed files for possible exporting.\n","\n","## Install Dependencies and Stable Baselines3 Using Pip"]},{"cell_type":"code","metadata":{"id":"YFdlFByORUnl"},"source":["!apt install swig\n","!pip install stable-baselines3[extra]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"grXe85G9RUnp","executionInfo":{"status":"ok","timestamp":1611343874043,"user_tz":360,"elapsed":3673,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["import gym\n","from stable_baselines3 import A2C, SAC, PPO, TD3"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hMPAn1SRd32f"},"source":["# Saving and loading\n","\n","Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."]},{"cell_type":"code","metadata":{"id":"vBNFnN4Gd32g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611343921416,"user_tz":360,"elapsed":12349,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"839cc55d-4ece-4191-ee42-ee0e3f6d0887"},"source":["import os\n","\n","# Create save dir\n","save_dir = \"/tmp/gym/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n","# The model will be saved under PPO_tutorial.zip\n","model.save(save_dir + \"/PPO_tutorial\")\n","\n","# sample an observation from the environment\n","obs = model.env.observation_space.sample()\n","\n","# Check prediction before saving\n","print(\"pre saved\", model.predict(obs, deterministic=True))\n","\n","del model # delete trained model to demonstrate loading\n","\n","loaded_model = PPO.load(save_dir + \"/PPO_tutorial\")\n","# Check that the prediction is the same after loading (for the same observation)\n","print(\"loaded\", loaded_model.predict(obs, deterministic=True))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["pre saved (array([0.13017756], dtype=float32), None)\n","loaded (array([0.13017756], dtype=float32), None)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gXWPrVqId32o"},"source":["Saving in stable-baselines is quite powerful, as you save the training hyperparameters, with the current weights. This means in practice, you can simply load a custom model, without redefining the parameters, and continue learning.\n","\n","The loading function can also update the model's class variables when loading."]},{"cell_type":"code","metadata":{"id":"LCtxrAbXd32q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611344026173,"user_tz":360,"elapsed":16578,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"dcbb7745-6d88-480d-c51a-876d0bcec325"},"source":["import os\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","\n","# Create save dir\n","save_dir = \"/tmp/gym/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n","# The model will be saved under A2C_tutorial.zip\n","model.save(save_dir + \"/A2C_tutorial\")\n","\n","del model # delete trained model to demonstrate loading\n","\n","# load the model, and when loading set verbose to 1\n","loaded_model = A2C.load(save_dir + \"/A2C_tutorial\", verbose=1)\n","\n","# show the save hyperparameters\n","print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps =\", loaded_model.n_steps)\n","\n","# as the environment is not serializable, we need to set a new instance of the environment\n","loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n","# and continue training\n","loaded_model.learn(8000)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["loaded: gamma = 0.9 n_steps = 20\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.a2c.a2c.A2C at 0x7f0c2c91a710>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"hKwupU-Jgxjm"},"source":["# Gym and VecEnv wrappers"]},{"cell_type":"markdown","metadata":{"id":"ds4AAfmISQIA"},"source":["## Anatomy of a gym wrapper"]},{"cell_type":"markdown","metadata":{"id":"gnTS9e9hTzZZ"},"source":["A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n","\n","Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n","There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"]},{"cell_type":"code","metadata":{"id":"hYo0C0TQSL3c","executionInfo":{"status":"ok","timestamp":1611344966970,"user_tz":360,"elapsed":288,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["class CustomWrapper(gym.Wrapper):\n","  \"\"\"\n","  :param env: (gym.Env) Gym environment that will be wrapped\n","  \"\"\"\n","  def __init__(self, env):\n","    # Call the parent constructor, so we can access self.env later\n","    super(CustomWrapper, self).__init__(env)\n","  \n","  def reset(self):\n","    \"\"\"\n","    Reset the environment \n","    \"\"\"\n","    obs = self.env.reset()\n","    return obs\n","\n","  def step(self, action):\n","    \"\"\"\n","    :param action: ([float] or int) Action taken by the agent\n","    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n","    \"\"\"\n","    obs, reward, done, info = self.env.step(action)\n","    return obs, reward, done, info\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zeGuyICUN26"},"source":["## First example: limit the episode length\n","\n","One practical use case of a wrapper is when you want to limit the number of steps by episode, for that you will need to overwrite the `done` signal when the limit is reached. It is also a good practice to pass that information in the `info` dictionnary."]},{"cell_type":"code","metadata":{"id":"Eb2U4_K6SNUx","executionInfo":{"status":"ok","timestamp":1611345090146,"user_tz":360,"elapsed":261,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["class TimeLimitWrapper(gym.Wrapper):\n","  \"\"\"\n","  :param env: (gym.Env) Gym environment that will be wrapped\n","  :param max_steps: (int) Max number of steps per episode\n","  \"\"\"\n","  def __init__(self, env, max_steps=100):\n","    # Call the parent constructor, so we can access self.env later\n","    super(TimeLimitWrapper, self).__init__(env)\n","    self.max_steps = max_steps\n","    # Counter of steps per episode\n","    self.current_step = 0\n","  \n","  def reset(self):\n","    \"\"\"\n","    Reset the environment \n","    \"\"\"\n","    # Reset the counter\n","    self.current_step = 0\n","    return self.env.reset()\n","\n","  def step(self, action):\n","    \"\"\"\n","    :param action: ([float] or int) Action taken by the agent\n","    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n","    \"\"\"\n","    self.current_step += 1\n","    obs, reward, done, info = self.env.step(action)\n","    # Overwrite the done signal when \n","    if self.current_step >= self.max_steps:\n","      done = True\n","      # Update the info dict to signal that the limit was exceeded\n","      info['time_limit_reached'] = True\n","    return obs, reward, done, info\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZufaUJwVM9w"},"source":["#### Test the wrapper"]},{"cell_type":"code","metadata":{"id":"szZ43D5PVB07","executionInfo":{"status":"ok","timestamp":1611345293857,"user_tz":360,"elapsed":274,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["from gym.envs.classic_control.pendulum import PendulumEnv\n","\n","# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n","env = PendulumEnv()\n","# Wrap the environment\n","env = TimeLimitWrapper(env, max_steps=100)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"cencka9iVg9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611345342649,"user_tz":360,"elapsed":291,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"ac3aef6e-1e52-4cde-ec13-05687c5da3a6"},"source":["obs = env.reset()\n","done = False\n","n_steps = 0\n","while not done:\n","  # Take random actions\n","  random_action = env.action_space.sample()\n","  obs, reward, done, info = env.step(random_action)\n","  n_steps += 1\n","\n","print(n_steps, info)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["100 {'time_limit_reached': True}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jkMYA63sV9aA"},"source":["In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."]},{"cell_type":"markdown","metadata":{"id":"VIIJbSyQW9R-"},"source":["## Second example: normalize actions\n","\n","It is usually a good idea to normalize observations and actions before giving it to the agent, this prevent [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n","\n","In this example, we are going to normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2].\n","\n","Note: here we are dealing with continuous actions, hence the `gym.Box` space"]},{"cell_type":"code","metadata":{"id":"F5E6kZfzW8vy","executionInfo":{"status":"ok","timestamp":1611345687868,"user_tz":360,"elapsed":300,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["import numpy as np\n","\n","class NormalizeActionWrapper(gym.Wrapper):\n","  \"\"\"\n","  :param env: (gym.Env) Gym environment that will be wrapped\n","  \"\"\"\n","  def __init__(self, env):\n","    # Retrieve the action space\n","    action_space = env.action_space\n","    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n","    # Retrieve the max/min values\n","    self.low, self.high = action_space.low, action_space.high\n","\n","    # We modify the action space, so all actions will lie in [-1, 1]\n","    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n","\n","    # Call the parent constructor, so we can access self.env later\n","    super(NormalizeActionWrapper, self).__init__(env)\n","  \n","  def rescale_action(self, scaled_action):\n","      \"\"\"\n","      Rescale the action from [-1, 1] to [low, high]\n","      (no need for symmetric action space)\n","      :param scaled_action: (np.ndarray)\n","      :return: (np.ndarray)\n","      \"\"\"\n","      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n","\n","  def reset(self):\n","    \"\"\"\n","    Reset the environment \n","    \"\"\"\n","    # Reset the counter\n","    return self.env.reset()\n","\n","  def step(self, action):\n","    \"\"\"\n","    :param action: ([float] or int) Action taken by the agent\n","    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n","    \"\"\"\n","    # Rescale action from [-1, 1] to original [low, high] interval\n","    rescaled_action = self.rescale_action(action)\n","    obs, reward, done, info = self.env.step(rescaled_action)\n","    return obs, reward, done, info\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TmJ0eahNaR6K"},"source":["#### Test before rescaling actions"]},{"cell_type":"code","metadata":{"id":"UEnjBwisaQIx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611345735679,"user_tz":360,"elapsed":311,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"5ab222ac-78da-4424-ea02-d8cc3bb77195"},"source":["original_env = gym.make(\"Pendulum-v0\")\n","\n","print(original_env.action_space.low)\n","for _ in range(10):\n","  print(original_env.action_space.sample())"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[-2.]\n","[0.95998704]\n","[0.21297975]\n","[1.3062631]\n","[1.8154578]\n","[-1.5473017]\n","[0.54342765]\n","[-1.646277]\n","[0.34432086]\n","[0.40397424]\n","[-1.8924892]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jvcll2L3afVd"},"source":["#### Test the NormalizeAction wrapper"]},{"cell_type":"code","metadata":{"id":"WsCM9AUGaeBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611345765032,"user_tz":360,"elapsed":311,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"f95700a3-518f-407a-c271-7b9e830e6f7b"},"source":["env = NormalizeActionWrapper(gym.make(\"Pendulum-v0\"))\n","\n","print(env.action_space.low)\n","\n","for _ in range(10):\n","  print(env.action_space.sample())"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[-1.]\n","[-0.04684554]\n","[-0.526255]\n","[0.3462018]\n","[0.40019763]\n","[0.6826863]\n","[0.5160603]\n","[-0.83484614]\n","[-0.555765]\n","[0.0800269]\n","[0.42097747]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V5h5kk2mbGNs"},"source":["#### Test with a RL algorithm\n","\n","We are going to use the Monitor wrapper of stable baselines, wich allow to monitor training stats (mean episode reward, mean episode length)"]},{"cell_type":"code","metadata":{"id":"R9FNCN8ybOVU","executionInfo":{"status":"ok","timestamp":1611345854497,"user_tz":360,"elapsed":266,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.vec_env import DummyVecEnv"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"wutM3c1GbfGP","executionInfo":{"status":"ok","timestamp":1611345855929,"user_tz":360,"elapsed":228,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["env = Monitor(gym.make('Pendulum-v0'))\n","env = DummyVecEnv([lambda: env])"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cxnE5bdaQ_3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611345858701,"user_tz":360,"elapsed":1971,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"d0fbbcdf-041e-4910-c1a3-b84fdf3b2b04"},"source":["model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Using cpu device\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EJFSM-Drb3Wc"},"source":["With the action wrapper"]},{"cell_type":"code","metadata":{"id":"GszFZthob2wM","executionInfo":{"status":"ok","timestamp":1611345895494,"user_tz":360,"elapsed":317,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["normalized_env = Monitor(gym.make('Pendulum-v0'))\n","# Note that we can use multiple wrappers\n","normalized_env = NormalizeActionWrapper(normalized_env)\n","normalized_env = DummyVecEnv([lambda: normalized_env])"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrKJEO4NcIMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611345902542,"user_tz":360,"elapsed":2122,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"3653cc77-7789-4cd6-ce2f-d75b4701deb7"},"source":["model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Using cpu device\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5BxqXd_6dpJx"},"source":["## Additional wrappers: VecEnvWrappers\n","\n","In the same vein as gym wrappers, stable baselines provide wrappers for `VecEnv`. Among the different that exist (and you can create your own), you should know: \n","\n","- VecNormalize: it computes a running mean and standard deviation to normalize observation and returns\n","- VecFrameStack: it stacks several consecutive observations (useful to integrate time in the observation, e.g. sucessive frame of an atari game)\n","\n","More info in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#wrappers)\n","\n","Note: when using `VecNormalize` wrapper, you must save the running mean and std along with the model, otherwise you will not get proper results when loading the agent again. If you use the [rl zoo](https://github.com/DLR-RM/rl-baselines3-zoo), this is done automatically"]},{"cell_type":"code","metadata":{"id":"zuIcbfv3g9dd","executionInfo":{"status":"ok","timestamp":1611346288062,"user_tz":360,"elapsed":278,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n","\n","env = DummyVecEnv([lambda: gym.make(\"Pendulum-v0\")])\n","normalized_vec_env = VecNormalize(env)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PAbu21pg90A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611346289466,"user_tz":360,"elapsed":341,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"bc4936ea-6684-441b-ce07-123fb1d774c2"},"source":["obs = normalized_vec_env.reset()\n","for _ in range(10):\n","  action = [normalized_vec_env.action_space.sample()]\n","  obs, reward, _, _ = normalized_vec_env.step(action)\n","  print(obs, reward)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[[-0.00183561  0.00700768  0.00888749]] [-1.9999754]\n","[[-0.99267316 -0.8326871   0.9980975 ]] [-1.3473532]\n","[[-1.2772409 -1.3195938  1.3353181]] [-1.0658776]\n","[[-1.4193096 -1.5668793  1.4064068]] [-0.9724436]\n","[[-1.4597988 -1.7106047  1.3300911]] [-0.91271996]\n","[[-1.4265751 -1.8021103  1.3045003]] [-0.83927983]\n","[[-1.3036867 -1.8353571  1.1389475]] [-0.78089786]\n","[[-1.076632  -1.8503325  1.1834317]] [-0.7114392]\n","[[-0.6971241 -1.820186   1.0270087]] [-0.57496005]\n","[[-0.18203856 -1.7375184   0.5511608 ]] [-0.4247158]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UEpTys28Wz05"},"source":["## Exercise: code you own monitor wrapper\n","\n","Now that you know how does a wrapper work and what you can do with it, it's time to experiment.\n","\n","The goal here is to create a wrapper that will monitor the training progress, storing both the episode reward (sum of reward for one episode) and episode length (number of steps in for the last episode).\n","\n","You will return those values using the `info` dict after each end of episode."]},{"cell_type":"code","metadata":{"id":"8FWeDRd5W7hO","executionInfo":{"status":"ok","timestamp":1611348071232,"user_tz":360,"elapsed":304,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["class MyMonitorWrapper(gym.Wrapper):\n","  \"\"\"\n","  :param env: (gym.Env) Gym environment that will be wrapped\n","  \"\"\"\n","  def __init__(self, env):\n","    # Call the parent constructor, so we can access self.env later\n","    super(MyMonitorWrapper, self).__init__(env)\n","    # === YOUR CODE HERE ===#\n","    # Initialize the variables that will be used\n","    # to store the episode length and episode reward\n","    self.reward_sum = 0\n","    self.episode_length = 0\n","    # ====================== #\n","  \n","  def reset(self):\n","    \"\"\"\n","    Reset the environment \n","    \"\"\"\n","    obs = self.env.reset()\n","    # === YOUR CODE HERE ===#\n","    # Reset the variables\n","    self.reward_sum = 0\n","    self.episode_length = 0\n","    # ====================== #\n","    return obs\n","\n","  def step(self, action):\n","    \"\"\"\n","    :param action: ([float] or int) Action taken by the agent\n","    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n","    \"\"\"\n","    obs, reward, done, info = self.env.step(action)\n","    # === YOUR CODE HERE ===#\n","    # Update the current episode reward and episode length\n","    self.reward_sum += reward\n","    self.episode_length += 1\n","    # ====================== #\n","\n","    if done:\n","      # === YOUR CODE HERE ===#\n","      # Store the episode length and episode reward in the info dict\n","      info = self.episode_length, self.reward_sum\n","\n","      # ====================== #\n","    return obs, reward, done, info"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4fY4QwWXNFK"},"source":["#### Test your wrapper"]},{"cell_type":"code","metadata":{"id":"bJbUG-A_liYt"},"source":["# To use LunarLander, you need to install box2d box2d-kengz (pip) and swig (apt-get)\n","!pip install box2d box2d-kengz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWZp1olSXMUg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611348073356,"user_tz":360,"elapsed":266,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"cea22742-e08e-4650-b221-1159f17c61c4"},"source":["env = gym.make(\"LunarLander-v2\")\n","# === YOUR CODE HERE ===#\n","# Wrap the environment\n","env = MyMonitorWrapper(env)\n","# Reset the environment\n","obs = env.reset()\n","# Take random actions in the enviromnent and check\n","# that it returns the correct values after the end of each episode\n","done = False\n","n_steps = 0\n","while not done:\n","  # Take random actions\n","  random_action = env.action_space.sample()\n","  obs, reward, done, info = env.step(random_action)\n","  n_steps += 1\n","\n","print(n_steps, info)\n","# ====================== #"],"execution_count":41,"outputs":[{"output_type":"stream","text":["113 (113, -112.74824692111059)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dJ2IqSM2eOt8"},"source":[" # Conclusion\n"," \n"," In this notebook, we have seen:\n"," - how to easily save and load a model\n"," - what is wrapper and what we can do with it\n"," - how to create your own wrapper"]},{"cell_type":"markdown","metadata":{"id":"qhWB_bHpSkas"},"source":["## Wrapper Bonus: changing the observation space: a wrapper for episode of fixed length"]},{"cell_type":"code","metadata":{"id":"bBlS9YxYSpJn"},"source":["from gym.wrappers import TimeLimit\n","\n","class TimeFeatureWrapper(gym.Wrapper):\n","    \"\"\"\n","    Add remaining time to observation space for fixed length episodes.\n","    See https://arxiv.org/abs/1712.00378 and https://github.com/aravindr93/mjrl/issues/13.\n","\n","    :param env: (gym.Env)\n","    :param max_steps: (int) Max number of steps of an episode\n","        if it is not wrapped in a TimeLimit object.\n","    :param test_mode: (bool) In test mode, the time feature is constant,\n","        equal to zero. This allow to check that the agent did not overfit this feature,\n","        learning a deterministic pre-defined sequence of actions.\n","    \"\"\"\n","    def __init__(self, env, max_steps=1000, test_mode=False):\n","        assert isinstance(env.observation_space, gym.spaces.Box)\n","        # Add a time feature to the observation\n","        low, high = env.observation_space.low, env.observation_space.high\n","        low, high= np.concatenate((low, [0])), np.concatenate((high, [1.]))\n","        env.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n","\n","        super(TimeFeatureWrapper, self).__init__(env)\n","\n","        if isinstance(env, TimeLimit):\n","            self._max_steps = env._max_episode_steps\n","        else:\n","            self._max_steps = max_steps\n","        self._current_step = 0\n","        self._test_mode = test_mode\n","\n","    def reset(self):\n","        self._current_step = 0\n","        return self._get_obs(self.env.reset())\n","\n","    def step(self, action):\n","        self._current_step += 1\n","        obs, reward, done, info = self.env.step(action)\n","        return self._get_obs(obs), reward, done, info\n","\n","    def _get_obs(self, obs):\n","        \"\"\"\n","        Concatenate the time feature to the current observation.\n","\n","        :param obs: (np.ndarray)\n","        :return: (np.ndarray)\n","        \"\"\"\n","        # Remaining time is more general\n","        time_feature = 1 - (self._current_step / self._max_steps)\n","        if self._test_mode:\n","            time_feature = 1.0\n","        # Optionnaly: concatenate [time_feature, time_feature ** 2]\n","        return np.concatenate((obs, [time_feature]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-vWgkZzd4F1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojn4nvNNRUoT"},"source":["## Going further - Saving format \n","\n","The format for saving and loading models is a zip-archived JSON dump and NumPy zip archive of the arrays:\n","```\n","saved_model.zip/\n","├── data              JSON file of class-parameters (dictionary)\n","├── parameter_list    JSON file of model parameters and their ordering (list)\n","├── parameters        Bytes from numpy.savez (a zip file of the numpy arrays). ...\n","    ├── ...           Being a zip-archive itself, this object can also be opened ...\n","        ├── ...       as a zip-archive and browsed.\n","```"]},{"cell_type":"markdown","metadata":{"id":"QWAcc8RFRUoU"},"source":["## Save and find "]},{"cell_type":"code","metadata":{"id":"4tcQxzSCRUoV","executionInfo":{"status":"ok","timestamp":1611348319145,"user_tz":360,"elapsed":13813,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}}},"source":["# Create save dir\n","save_dir = \"/tmp/gym/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n","model.save(save_dir + \"/PPO_tutorial\")"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGaMNz4HRUoX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611348322334,"user_tz":360,"elapsed":428,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"75253456-1388-42ef-f45b-dda967980d7c"},"source":["!ls /tmp/gym/PPO_tutorial*"],"execution_count":43,"outputs":[{"output_type":"stream","text":["/tmp/gym/PPO_tutorial.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gYY3nQyyRUoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611348325018,"user_tz":360,"elapsed":304,"user":{"displayName":"Andrew Albright","photoUrl":"","userId":"05166390234084678131"}},"outputId":"c07d9c51-76c7-4d4d-f316-57ab363556bd"},"source":["import zipfile\n","\n","archive = zipfile.ZipFile(\"/tmp/gym/PPO_tutorial.zip\", 'r')\n","for f in archive.filelist:\n","  print(f.filename)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["data\n","pytorch_variables.pth\n","policy.pth\n","policy.optimizer.pth\n","_stable_baselines3_version\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cPKkkTvjRUo2"},"source":["## Exporting saved models\n","\n","And finally some futher reading for those who want to export to tensorflowJS or Java.\n","\n","https://stable-baselines.readthedocs.io/en/master/guide/export.html"]}]}