Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Vaughan2013,
abstract = {Under certain conditions, hopping, skipping, or jumping might be a more efficient means of terrestrial locomotion than either walking or rolling. In addition, for many robotic applications, a robust jumping algorithm could be easier to implement than walking. Jumping also provides a means to cross obstacles that might otherwise stop a wheeled or walking robot. This paper will examine a special case of jumping and generation of jumping commands. A simplified flexible robotic leg is modeled and jumping commands are generated utilizing a command shaping algorithm.},
annote = {cite for control input},
author = {Vaughan, Joshua},
file = {:C$\backslash$:/Users/andre/Downloads/06695635.pdf:pdf},
isbn = {7777777777},
publisher = {IEEE},
title = {{Jumping Commands For Flexible-Legged Robots}},
year = {2013}
}
@article{Ha2019,
abstract = {In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agentʼs physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agentʼs design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications.},
annote = {From Vaughan},
archivePrefix = {arXiv},
arxivId = {1810.03779},
author = {Ha, David},
doi = {10.1162/artl_a_00301},
eprint = {1810.03779},
file = {:C$\backslash$:/Users/andre/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha - 2019 - Reinforcement learning for improving agent design(2).pdf:pdf},
issn = {15309185},
journal = {Artificial Life},
keywords = {Deep reinforcement learning,Evolution strategies,Generative design,Neuroevolution},
number = {4},
pages = {352--365},
pmid = {31697584},
title = {{Reinforcement learning for improving agent design}},
volume = {25},
year = {2019}
}
@article{Sugiyama2004,
abstract = {We describe crawling and jumping by a deformable soft robot. Locomotion over rough terrain has been achieved mainly by rigid body systems including crawlers and leg mechanisms. This paper presents an alternative method of moving over rough terrain, one that employs deformation. First, we describe the principle of crawling and jumping as performed through deformation of a robot body. Second, in a physical simulation, we investigate the feasibility of the approach. Next, we show experimentally that a prototype of a circular soft robot can crawl and jump.},
annote = {cite for flexible systems being advantagous},
author = {Sugiyama, Yuuta and Hirai, Shinichi},
doi = {10.1109/iros.2004.1389922},
file = {:C$\backslash$:/Users/andre/Downloads/01389922.pdf:pdf},
isbn = {0780384636},
journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
keywords = {Crawl,Deformation,Jump,Locomotion},
number = {c},
pages = {3276--3281},
title = {{Crawling and jumping of deformable soft robot}},
volume = {4},
year = {2004}
}
@article{Schaff2019a,
abstract = {The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learning-based approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical - i.e., by picking a design and training a control policy for it. Since training these policies is time-consuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that jointly optimizes over the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines across different settings.},
archivePrefix = {arXiv},
arxivId = {1801.01432},
author = {Schaff, Charles and Yunis, David and Chakrabarti, Ayan and Walter, Matthew R.},
doi = {10.1109/ICRA.2019.8793537},
eprint = {1801.01432},
file = {:C$\backslash$:/Users/andre/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaff et al. - 2019 - Jointly learning to construct and control agents using deep reinforcement learning.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {9798--9805},
title = {{Jointly learning to construct and control agents using deep reinforcement learning}},
volume = {2019-May},
year = {2019}
}
@article{Ghorbel1990,
author = {Ghorbel, Fathi and Hung, John Y and Spong, Mark W},
doi = {10.1109/ICSMC.1990.142189},
file = {:C$\backslash$:/Users/andre/Downloads/Adaptive{\_}control{\_}of{\_}flexible{\_}joint{\_}manipulators.pdf:pdf},
number = {December},
title = {{Adaptive Control of Flexible-Joint Manipulators}},
year = {1990}
}
@article{Auerbach2014,
abstract = {Whether, when, how, and why increased complexity evolves in biological populations is a longstanding open question. In this work we combine a recently developed method for evolving virtual organisms with an information-theoretic metric of morphological complexity in order to investigate how the complexity of morphologies, which are evolved for locomotion, varies across different environments. We first demonstrate that selection for locomotion results in the evolution of organisms with morphologies that increase in complexity over evolutionary time beyond what would be expected due to random chance. This provides evidence that the increase in complexity observed is a result of a driven rather than a passive trend. In subsequent experiments we demonstrate that morphologies having greater complexity evolve in complex environments, when compared to a simple environment when a cost of complexity is imposed. This suggests that in some niches, evolution may act to complexify the body plans of organisms while in other niches selection favors simpler body plans. {\textcopyright} 2014 Auerbach, Bongard.},
annote = {cite on ES being used to change a robot structures},
author = {Auerbach, Joshua E. and Bongard, Josh C.},
doi = {10.1371/journal.pcbi.1003399},
file = {:C$\backslash$:/Users/andre/Downloads/ContentServer.pdf:pdf},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {1},
pmid = {24391483},
title = {{Environmental Influence on the Evolution of Morphological Complexity in Machines}},
volume = {10},
year = {2014}
}

@online{Atlas,
  author = {Boston Dynamics},
  title = {{Atlas®} Boston Dynamics},
  year = 2021,
  url = {https://www.bostondynamics.com/atlas},
  urldate = {2021-03-20}
}

@online{Athlete,
  author = {designboom},
  title = {{ryuma niiyama} athlete robot},
  year = 2010,
  url = {https://www.bostondynamics.com/atlas},
  urldate = {2021-03-20}
}