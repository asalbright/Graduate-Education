Date: 11/18/2020    
Author: Andrew Albright    
Email: andrew.albright1@louisiana.edu

# Discussion of Key Findings
I haven't particularly found anything other than material over the past couple of weeks. I am really enjoying looking into material related to the idea of evolving a systems physical parameters which would typically be treated as hyperparameters in other RL work. 

I think there is a lot of room to do some evaluation and testing both in simulation and on real hardware for learning flexible systems. I have been focused on the research of both developing the physical aspects of a system as well as the controlling aspects. And I think there is some cool work to be looked into with mixing and matching RL and ES to learn both sides of the system so to speak.

I also worked through a bunch of your code that you sent on system development in python. I found the material in your 513 class to be really relatable to the stuff we have worked throughout this semester (which makes sense) and I will if I find them time model some of the problems we modeled in Mathematica but in Python.

# Current Difficulties
## Theoretical/Analytical Difficulties
### Evolution Strategies
I have an overhead type understanding of this idea right now. But I haven't spent a lot of time looking into a bunch of details mathematically. I will get around to it though.

## Technical/Implementation Difficulties
### Time During Finals
Just wrapping up final assignments, presentations, and projects for the semester. Also preparing for the finals I have.
# Team Activities
The team has slowed down here in the last bit of the semester. We are still working through assembly of the parts we do have in house, mainly focusing on the issues that we are finding along the way.

A preliminary design for the Tail Cutting subsystem has also been completed. We are working on determining some specifics related to the systems performance needs so that we can source some parts appropriately.

1. Brennon - working on assembling Tail Clamping subsystem    
    
2. Darcy - working on some issues found while assembling the Head/Tail Separation subsystem.

3. Myself - waiting for some last min parts to finish assembly of the Meat Removal subsystem & working on the design for the Tail Cutting subsystem.
    
# Papers
## High Level Reviews
1. Alghonaim, R., & Johns, E. (2020). Benchmarking Domain Randomisation for Visual Sim-to-Real Transfer. Retrieved from http://arxiv.org/abs/2011.07112
2. Tanwani, A. K. (2020). Domain-Invariant Representation Learning for Sim-to-Real Transfer. Retrieved from http://arxiv.org/abs/2011.07589
3. Salimans, T., Ho, J., Chen, X., Sidor, S., & Sutskever, I. (2017, March 10). Evolution strategies as a scalable alternative to reinforcement learning. ArXiv. arXiv.
4. Ha, D. (2019). Reinforcement learning for improving agent design. Artificial Life, 25(4), 352–365. https://doi.org/10.1162/artl_a_00301
5. Schaff, C., Yunis, D., Chakrabarti, A., & Walter, M. R. (2019). Jointly learning to construct and control agents using deep reinforcement learning. In Proceedings - IEEE International Conference on Robotics and Automation (Vol. 2019-May, pp. 9798–9805). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/ICRA.2019.8793537
6. Cheney, N., MacCurdy, R., Clune, J., & Lipson, H. (2013). Unshackling evolution: Evolving soft robots with multiple materials and a powerful generative encoding. In GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference (pp. 167–174). https://doi.org/10.1145/2463372.2463404
7. Ha, S., Xu, P., Tan, Z., Levine, S., & Tan, J. (2020, February 19). Learning to walk in the real world with minimal human effort. ArXiv. arXiv.
8. Kumar, V., Hoeller, D., Sundaralingam, B., Tremblay, J., & Birchfield, S. (2020). Joint Space Control via Deep Reinforcement Learning. Retrieved from http://arxiv.org/abs/2011.06332
9. Liang, J., & Kroemer, O. (2020). Contact Localization for Robot Arms in Motion without Torque Sensing. Retrieved from http://arxiv.org/abs/2011.03142


## Detailed Reviews

### Ha, D. (2019). Reinforcement learning for improving agent design. Artificial Life, 25(4), 352–365. https://doi.org/10.1162/artl_a_00301

### Summary
Develop a RL strategy where the agent parameters it can change are physical aspects of its body. Simulated on a few OpenAI locomotive environments, the parts of the body that could be changed are mainly leg physics related (length, width, radius, joint orientation and mass). Weights ω are treated as parameters which need to be learned to maximize the expected Reward. Testing was done as well where rewarding was increased for "trying more difficult designs". The average score received was higher for learned morphologies vs original morphologies. Beyond that, the standard deviation seen in the reward received over 100 trials was lower in the learned, compared the original design.

### Main Contribution to Relationship Field
Develops a method to learn system parameters which would regularly be treated as hyperparameters for the purpose of developing an optimal mechanical design for a task.

### Relevance to Current Research
This is what I am going to be looking to accomplish but in the direction of non-rigid systems. The ideas are very similar and starting with rigid systems is a good place to build a foundation.

### Salimans, T., Ho, J., Chen, X., Sidor, S., & Sutskever, I. (2017, March 10). Evolution strategies as a scalable alternative to reinforcement learning. ArXiv. arXiv.

### Summary
Discussion on the uses of ES as an alternative to RL strategies for solving tasks. Compares ES to RL to evaluate the pros and cons of ES. Computationally, ES seems to take the win because of its not needing to backpropagate. It is also perhaps a preferable method to use when the reward is often delayed for many time steps as RL methods might find bad strategies during this delay. Another benefit is that ES is highly parallelizable, which can make up for sparse data efficiency. It is an important thing to note though that the language they use in this paper is that "ES, when **carefully** implemented, is competitive with Rl algorithms...". 

### Main Contribution to Relationship Field
(copied from paper)

1. We found that the use of virtual batch normalization [Salimans et al., 2016] and other reparameterizations of the neural network policy (section 2.2) greatly improve the reliability of evolution strategies. Without these methods ES proved brittle in our experiments, but with these reparameterizations we achieved strong results over a wide variety of environments.
2. We found the evolution strategies method to be highly parallelizable: by introducing a novel communication strategy based on common random numbers, we are able to achieve linear speedups in run time even when using over a thousand workers. In particular, using 1,440 workers, we have been able to solve the MuJoCo 3D humanoid task in under 10 minutes.
3. The data efficiency of evolution strategies was surprisingly good: we were able to match the final performance of A3C [Mnih et al., 2016] on most Atari environments while using between 3x and 10x as much data. The slight decrease in data efficiency is partly offset by a
reduction in required computation of roughly 3x due to not performing backpropagation and not having a value function. Our 1-hour ES results require about the same amount of computation as the published 1-day results for A3C, while performing better on 23 games tested, and worse on 28. On MuJoCo tasks, we were able to match the learned policy performance of Trust Region Policy Optimization [TRPO; Schulman et al., 2015], using no more than 10x as much data.
4. We found that ES exhibited better exploration behaviour than policy gradient methods like TRPO: on the MuJoCo humanoid task, ES has been able to learn a very wide variety of gaits (such as walking sideways or walking backwards). These unusual gaits are never observed with TRPO, which suggests a qualitatively different exploration behavior.
5. We found the evolution strategies method to be robust: we achieved the aforementioned results using fixed hyperparameters for all the Atari environments, and a different set of fixed hyperparameters for all MuJoCo environments (with the exception of one binary hyper- parameter, which has not been held constant between the different MuJoCo environments).

### Relevance to Current Research
This is a new-ish topic for me this report and I plan to learn more about ES and their potential uses in comparing the outcomes to RL strategies.

### Schaff, C., Yunis, D., Chakrabarti, A., & Walter, M. R. (2019). Jointly learning to construct and control agents using deep reinforcement learning. In Proceedings - IEEE International Conference on Robotics and Automation (Vol. 2019-May, pp. 9798–9805). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/ICRA.2019.8793537

### Summary
Introduces a RL algorithm which "jointly optimizes over both physical design and control". The goal of the work is to create an algorithm (PPO) which will find an optimal ω* to be used in conjunction with an optimal π* to accomplish a locomotive type task.Simulation results show that starting from a "random initialization" the algorithm is able to find physical designs and gates on several locomotive environments within OpenAI which outperform controllers learned on standard static system parameters.
### Main Contribution to Relationship Field
Shows that utilizing a RL strategy to change a systems physical and typically non-parameterizable parameters, along side the development of a controller for the optimized morphology can yield strong results. More than that these results can be found to be better than just allowing a RL strategy to optimize a controller for a set mechanical design.
### Relevance to Current Research
This paper illustrates the current direction I am looking to take but with rigid system dynamics. I plan on finding more papers like this and using them to build from.

## Other Reading/Research
I have also taken a look, not exhaustively yet, at all of the links you sent after our conversation regarding the Running/Jumping Robots. They are in my list of To-Dos still and are getting checked off. They are more blog-type though so they didn't make the papers list!

## Future Reading
### Types of papers I plan to read in the next two weeks:
* Papers related to learning physical system parameters along side control strategies. Also papers along the ES path of learning how to control/build a system. 

### What I am aiming to achieve from reading these papers: 
* Learn the approaches others have taken in treating system hyperparameters as parameters. See if I can find work that others have done in trying to learn physical parameters on a non-rigid system.

### Why I am looking to achieve that:
* So that I can find some work to try and *replicate* so to speak before I start experimenting on my own.

# Plan for the Next Two Weeks
(Same as last time)

At a glance, get through finals week! These next couple of weeks are going to be a bit jammed trying to study up and make sure I am prepared to do well during finals week. 

Beyond that I will be working more directly on some of the material related to the Running/Jumping Robots. I will also be looking to see when the Crawfish Peeling team will have time in between their finals week studies to get some assembly done. 

## Ideas Looking Forward

### Crawfish Peeler Project
We are looking into some physical aspects of a crawfish tail so we can determine a path which a Tail Cutting subsystem would have to follow. I am thinking once we identify the numbers that the team gathered a while back, we can develop that path and add some tolerance in a conservative way and source some parts.

### Robotics Research
After doing a bit of reading in the area of Learning physical parameters along side a control strategy I am thinking in the future about how I might be able to utilize ES to do one or both of those things as well. I think it would be interesting to build a learning strategy which uses ES to learn the physical parameters and RL to learn the controller, then vise versa, then both using RL, then both using ES. I think this could all be done on a the single legged system and the data could be presented in a paper. Perhaps as a setup to that though just getting the system up and running using one of the strategies and gathering the sim-to-real data would be great and also paper worthy.

## Current Next Steps

### Crawfish Peeler Project
Get the parts we have assembled and operating. Continue a forward moving design of the Tail Cutting subsystem.

### Robotics Research
Continue finding work accomplished in the area of RL utilized in physical design along side controller design. Begin work with the models that exist for the single legged system.

## Expectations for Next Report (12/02/2020)
1. Continue to work on the Running/Jumping Robot material you sent in the email on Friday (11/06)
2. Re-write some of the RL algorithms I think I might be using to do some work
    * This is still on my list, as again I think it will be beneficial

Because we are going into Finals week and then into Thanksgiving, I do not expect to have a ton of work done in the next two weeks. I am looking to finish the semester well in my classes and to set myself up to do some work over the break.

## Current Schedule Outlook
* 11/19 - Running/Jumping Meeting
* 11/20 - Team Check-In
* 11/23 - Crawfish Peeling Meeting
* 11/24 - MCHE 513 / ENGR 501 Final Exams
* 11/25 - Crawfish Bi-Weekly Report Due
* 11/26 - Running/Jumping Meeting
* 11/27 - Team Check-In
* 11/30 - Crawfish Peeling Meeting
* 12/02 - Next Report Due

# Long-term planning

## Upcoming Paper Deadlines
Would like to have a paper written with some results from a sim-to-real experiment using the single legged system in the lab. I am thinking bringing Logan on board for this paper would be great too as he seems to be interested in the RL side of things.

## Administrative Deadlines
### Crawfish Peeler
* No immediate deadlines. However, we are thinking forward knowing the next report for the Board will be the first quarter of next year.

### Research
[MS Graduation Process](http://crawlab.org/wiki/index.php?title=MS_Graduation_Process)