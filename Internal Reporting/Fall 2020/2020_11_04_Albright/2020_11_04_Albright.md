Date: 11/04/2020    
Author: Andrew Albright    
Email: andrew.albright1@louisiana.edu

# Discussion of Key Findings
After our discussion of Friday (11/06) I think we have found a direction for me to take as far as giving me a more intentional learning approach to figuring out more about RL.

I have found as well, after discussing some with Gerald that it isn't as easy as just linking Colab to GitHub. The method I have used this far is keeping it on my Google Drive and downloading at the end of a working session and uploading it to Git.

> ***JV*** I think it *is* easy if you link to a complete repository, rather than just a folder. However, your method seems okay for now.

Found that I am not totally sure what the file is that was being discussed for changing the default changes for getting "pretty" graphs in Py. 

> ***JV*** Let's say Python, no Py.

> ***AA*** Will do.

I understand the file which walks through changing all the parameters in the file, but the RC file is still a mystery to me.

> ***JV*** Have you looked at the instructions/explanation in the [matplotlib documentation](https://matplotlib.org/3.2.1/tutorials/introductory/customizing.html#the-matplotlibrc-file)?

> ***AA*** I have actually yes. I didn't read it in great detail though, it was just something I made my way to after it was brought up in the chat the other day. I suppose my question is: do I need to change the rc "example" file to the one that you have? You mentioned having to change line 33 as well. Just want to clarify if I need to use the RC file you provided.

# Current Difficulties

## Theoretical/Analytical Difficulties

## Technical/Implementation Difficulties
### Using Google Colab
I am honestly not totally sure how Colab works. Specifically how the file imports work. I should say this is more of a question than an issue, too. However I could see it sprouting into an issue, so I am asking.

Do they (the files/modules) need to be on my machine for me to import them to Colab? I haven't run into any issues yet, I don't think, as far as importing things, though I have seen messages letting me know certain things were satisfied by documents that are along file paths on my machine.

> ***JV***  There are certain packages available by default, mostly those that are common to using Tensorflow and Keras (since Colab is a Google product, like Tensorflow.) Others can typically be easily installed via `pip`. You can also run `pip freeze` to get the list of packages available by default. (You may need to use `!pip freeze`. The leading `!` tells Colab that `pip` is not a Python command. You do a similar thing in IPython or the Jupyter Notebook.) [This page](https://medium.com/lean-in-women-in-tech-india/google-colab-the-beginners-guide-5ad3b417dfa) is a decent beginner's overview. One of the [Getting Started notebooks](https://colab.research.google.com/notebooks/snippets/importing_libraries.ipynb) also includes that info, linked from [the first Getting Started notebook](https://colab.research.google.com/notebooks/intro.ipynb#scrollTo=-Rh3-Vt9Nev9).

> ***AA*** I will take a look at these. Using the `pip install` is where I got the message that the thing I was trying to install was already satisfied, too. 

# Team Activities
We are all currently getting lined up for finals week and all that entails. Brennon in particular is really busy and it is holding up his time to devote to assembly of the parts we have in house. When we get a bit of breathing room, 

> ***JV*** Given the break and holidays, we have less breathing room/time than you guys think, I think.

> ***AA*** Alright, we can discuss more later this evening, too.

we are going to be looking to get the parts we have assembled and running.

1. Brennon - working on making time.
    * Printed some dummy tails for us to get an idea as to how a tail might fit into the clamps current design    
    
2. Darcy - has begun assembly on the Head/Tail Separation subsystem with the parts we have in house, and is looking to order some parts to finalize this process.

3. Myself - has gotten some parts re-designed and re-cut for the dynamic cam system for the Meat Removal subsystem. Gotten parts ordered to accommodate these parts as well as some steel parts for supplements for the AL parts we currently have.

4. We are all working on getting together an initial concept design for the tail cutting device

> ***JV*** I'm looking forward to hearing more about all of this tonight.
    
# Papers
## High Level Reviews
1. T. G. Thuruthel, E. Falotico, F. Renda and C. Laschi, "Model-Based Reinforcement Learning for Closed-Loop Dynamic Control of Soft Robotic Manipulators," in IEEE Transactions on Robotics, vol. 35, no. 1, pp. 124-134, Feb. 2019, doi: 10.1109/TRO.2018.2878318.
2. Todorov, E., Erez, T., & Tassa, Y. (2012). MuJoCo: A physics engine for model-based control. In IEEE International Conference on Intelligent Robots and Systems (pp. 5026–5033). https://doi.org/10.1109/IROS.2012.6386109
3. Erez, T., Tassa, Y., & Todorov, E. (2015). Simulation tools for model-based robotics: Comparison of Bullet, Havok, MuJoCo, ODE and PhysX. In Proceedings - IEEE International Conference on Robotics and Automation (Vol. 2015-June, pp. 4397–4404). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/ICRA.2015.7139807
4. Kawaharazuka, K., Ogawa, T., & Nabeshima, C. (2019). Dynamic Task Control Method of a Flexible Manipulator Using a Deep Recurrent Neural Network. In IEEE International Conference on Intelligent Robots and Systems (pp. 7695–7701). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/IROS40897.2019.8967923
5. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). OpenAI Gym. Retrieved from http://arxiv.org/abs/1606.01540
6. He, W., Gao, H., Zhou, C., Yang, C., & Li, Z. (2020). Reinforcement Learning Control of a Flexible Two-Link Manipulator: An Experimental Investigation. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 1–11. https://doi.org/10.1109/tsmc.2020.2975232
7. Srinivasan, K., Eysenbach, B., Ha, S., Tan, J., & Finn, C. (2020). Learning to be Safe: Deep RL with a Safety Critic. Retrieved from http://arxiv.org/abs/2010.14603
8. Howison, T., Hauser, S., Hughes, J., & Iida, F. (2020). Reality-assisted evolution of soft robots through large-scale physical experimentation: a review. Retrieved from http://arxiv.org/abs/2009.13960
9. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2018). Deep reinforcement learning that matters. In 32nd AAAI Conference on Artificial Intelligence, AAAI 2018 (pp. 3207–3214). AAAI press.
10. Bas-Serrano, J., Curi, S., Krause, A., & Neu, G. (2020). Logistic $Q$-Learning. Retrieved from http://arxiv.org/abs/2010.11151
11. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236
12. Clark, J. E., Cham, J. G., Bailey, S. A., Froehlich, E. M., Nahata, P. K., Full, R. J., & Cutkosky, M. R. (2001). Biomimetic design and fabrication of a hexapedal running robot. Proceedings - IEEE International Conference on Robotics and Automation, 4, 3643–3649. https://doi.org/10.1109/ROBOT.2001.933183
13. Ajay, A., Kumar, A., Agrawal, P., Levine, S., & Nachum, O. (2020). OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning. Retrieved from http://arxiv.org/abs/2010.13611
14. Pradhan, S. K., & Subudhi, B. (2012). Real-time adaptive control of a flexible manipulator using reinforcement learning. IEEE Transactions on Automation Science and Engineering, 9(2), 237–249. https://doi.org/10.1109/TASE.2012.2189004
15. Cui, L., Chen, W., Wang, H., & Wang, J. (2019). Control of Flexible Manipulator Based on Reinforcement Learning. In Proceedings 2018 Chinese Automation Congress, CAC 2018 (pp. 2744–2749). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/CAC.2018.8623788
16. Molnar, C., Casalicchio, G., & Bischl, B. (2020). Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges. Retrieved from http://arxiv.org/abs/2010.09337
17. Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., & Pathak, D. (2020). Planning to Explore via Self-Supervised World Models. Retrieved from http://arxiv.org/abs/2005.05960


## Detailed Reviews

### Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). OpenAI Gym. Retrieved from http://arxiv.org/abs/1606.01540

### Summary
Paper describing the basics behind OpenAi Gym. Describes the environment as one in which the agent is to "experience" a series of episodes, each where the action taken in the environment is samples from a random distribution and the goal is to maximize the reward seen from the actions taken. Doesn't describe the build of the agent, but rather the interface of the agent to the environment. 

### Main Contribution to Relationship Field
Deploys a library of environments which researchers can utilize to test RL algorithms on a set of "benchmark" or perhaps more advanced set of environments. 

### Relevance to Current Research
This is something that is utilized in the lab a bit, I was told, so I thought I might try and expand my understanding as to what it is. I probably could have found this out by reading a blog of sorts, but I found this paper and I figured why not read it all the way through.


## Source He, W., Gao, H., Zhou, C., Yang, C., & Li, Z. (2020). Reinforcement Learning Control of a Flexible Two-Link Manipulator: An Experimental Investigation. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 1–11. https://doi.org/10.1109/tsmc.2020.2975232

### Summary
Developed a RL control algorithm to validate RL's use in the field of flexible robotics. The system used is a flexible 2-link planar manipulator (both links flexible). Utilized an actor-critic RL algorithm. Compared the developed RL control strategy to a PD control strategy. 

### Main Contribution to Relationship Field
Showed that by utilizing an RL control strategy, reduction in system vibration over a PD control strategy can be achieved, especially when high precision is required. This proving that further research in the area of using RL to control flexible linked robotic systems is viable and has potential for solid results.

> ***JV*** What RL method did they use? 

> ***AA*** They utilized an actor-critic type approach. "The actor-critic algorithm is an adaptive iterative method which consists of a strategy evaluation section and a strategy improvement section. The actor neural network uses radial basis function (RBF) NN to gradually accumulate the system experience to generate the appropriate control strategy, and the critic neural network is used to approximate the evaluation function for the current strategy." 

### Relevance to Current Research
Relevant in that this more directly correlates to what I want to work on as far as application is concerned. I am interested in the use of RL in conjunction with flexible-linked systems and this is a case in which that is being investigated. 

As a side note, there is not a lot of work out there on flexible systems and RL being worked on together. That is a statement made too, with not too much looking into this path directly, but still based on the looking I have done, the work in limited for sure.

> ***JV*** That's good. We should be able to make a significant contribution then.

> ***AA*** That's right!
 
### Source Srinivasan, K., Eysenbach, B., Ha, S., Tan, J., & Finn, C. (2020). Learning to be Safe: Deep RL with a Safety Critic. Retrieved from http://arxiv.org/abs/2010.14603

### Summary
Develops an approach for a policy to learn to take safe actions by which a "safty-critic" is created which can be utilized to allow for "fine-tuning" learning on a task where then the likelihood of taking a catastrophic action is limited. The "safety-critic", not unlike any other critic in an actor-critic algorithm, develops a probability relationship paired with an action to be taken in a given state. However, in this instance the probability by which the actor chooses an action by is not one of maximising reward, though in minimizing failure.

### Main Contribution to Relationship Field
"The primary contribution of this work is a framework for safe reinforcement learning, by learning safety precautions from previous experience."

### Relevance to Current Research
This paper was suggested by you, first of all. Secondly I find it an interesting approach to learning a task in general. It's relevance is primarily to expand my knowledge and understanding of RL as I learn more about the topic.

You also made mention in the email, which this paper was linked, about us thinking about how this idea might be implemented to train a policy for stability rather than safety. I think too, that this is a totally plausible idea and something that could be explored as a way to develop a controller where stability is a big issue. 

### Kawaharazuka, K., Ogawa, T., & Nabeshima, C. (2019). Dynamic Task Control Method of a Flexible Manipulator Using a Deep Recurrent Neural Network. In IEEE International Conference on Intelligent Robots and Systems (pp. 7695–7701). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/IROS40897.2019.8967923

> ***JV*** ?

> ***AA*** Whoops, filled these in from Mendeley and I must have missed the source. See above.

### Summary
Develop a learning control method to achieve dynamic tasks utilizing a non-rigid linked system. Utilizes sound and visual input/feedback to compute the robot state

> ***JV***  What kind of sound(s) do they use? 

> ***AA*** The task was beating a drum (a Wadaiko drum which is a traditional Japanese instrument), so the sound of the drum was sampled with a mic placed in the environment. Again the sound was used to determine the state of the task, or whether or not the task was being accomplished. A camera was also used to polarize an image of the drum against a black background to aid in determining the state of the robot while I am explaining some specifics.

and task state to determine the outcome of the control strategy and makes real-time changes to accomplish the task. Shows that by utilizing the various input types and utilizing their Dynamic Task Execution Network (DTXNET) is a viable solution for controlling non-rigid systems.

### Main Contribution to Relationship Field
(Bullet points from the paper)

The detailed contributions of this study are,
* Structure of DTXNET using image information to realize dynamic tasks by a flexible manipulator.
* Real-time control method to calculate optimized torque command using DTXNET and its backpropagation through time
* Realization of a dynamic task of handling sparse event information.
Realization of a dynamic task of handling sparse event information.

### Relevance to Current Research
An interesting approach to controlling non-rigid systems. Really it was just something I found while looking for work on RL being used to develop controllers for non-rigid systems. It peaked my interest more than anything.

## Future Reading
### Types of papers I plan to read in the next two weeks:
* I will be looking to read papers more directly related to RL in conjunction with non-rigid systems. That as well as looking into what we discussed in our meeting. Which, if I am understanding it correctly, is the idea of using RL to develop a controller which is optimized and parameterizable based on a set of physical parameters such as link stiffness. 

> ***JV*** The *concurrent*, or at least tightly-coupled,  solution of optimize leg parameters and control is the angle that we discussed.

> ***JV*** Please also take a look at the Evolution Algorithm papers/links that I sent. I think that path might be a great fit for the mechanical piece of the solution.

> ***AA*** I have begun looking at the material you sent. ^^ This is something that is really interesting and seems really nicely suited for a mechanical approach to utilizing RL, too.

### What I am aiming to achieve from reading these papers: 
* Grow, and more importantly, guide my understanding towards ideas and developments that are relevant to what I will be working on for my thesis work.

> ***JV*** Let's start being more detailed/specific here. What *techincal* questions are you hoping to answer? What *specifically* are you hoping to learn? What hole in your current work are you hoping that this reading address?

> ***AA*** Sure. Currently I am looking to learn about how to apply some of techniques discussed in [link](https://designrl.github.io/) which you sent in the email last Friday. Now that we have nailed down a more defined goal for my work, the hole I am trying to fill in my knowledge in really more of a direction choice. I am aiming to try and become more familiar with RL being used to learn and define system parameters which for the case of a non-rigid system would be something like link rigidity.

### Why I am looking to achieve that:
* So that I can begin with some hands on work including actual hardware in the lab. Allowing firstly, for my knowledge to better solidify, and secondly, to produce results to guide my work.

# Plan for the Next Two Weeks
At a glance, get through finals week! These next couple of weeks are going to be a bit jammed trying to study up and make sure I am prepared to do well during finals week. 

Beyond that I will be working more directly on some of the material related to the Running/Jumping Robots. I will also be looking to see when the Crawfish Peeling team will have time in between their finals week studies to get some assembly done. 

## Ideas Looking Forward

### Crawfish Peeler Project
This project has slowed down a lot as we come into the end of the semester. I know the guys are working hard on a lot of other tasks right now, but we are trying to keep the Tail Cutting subsystem on the mind still as it is something we have to get finished soon.

> ***JV*** There is *much* less time than you guys think, I think.

> ***AA*** You are right. I will be putting more time in this project in the near future. I have plans over the Thanksgiving holidays too, to spend all of my time on both the Crawfish Peeler and my research. I will be staying is Lafayette so I will have time to focus up.

### Robotics Research
Per our discussion on Friday (11/06), I will be moving towards developing knowledge on utilizing RL to develop learning methods for non-rigid systems focusing on optimization and parameterization of non-rigid parameters such a link stiffness.

## Current Next Steps

### Crawfish Peeler Project
Get the parts we have assembled and operating. Get a forward moving design of the Tail Cutting subsystem.

### Robotics Research
(Copied from above as my ideas this semester have all been related to what I am going to do next.)
Per our discussion on Friday (11/06), I will be moving towards developing knowledge on utilizing RL to develop learning methods for non-rigid systems focusing on optimization and parameterization of non-rigid parameters such a link stiffness.

## Expectations for Next Report (11/18/2020)
1. Begin work on the Running/Jumping Robot material you sent in the email on Friday (11/06)
    * begin building a map to determine the specifics of what I will be working on
    
> ***JV*** What *results* do will you have by the next report?

> ***AA*** Depending on what is currently available from the Running/Jumping Project, ideally I would like to get the 2-D planar mechanism in simulation in conjunction with some RL. I have not had a chance to take a look at what exists though. Is there an environment made for this system? 

1. Learn how to model a system utilizing Python
    * Didn't have time to do this yet. However it remains on my list as I think it will very beneficial
    
> ***JV*** You should get to this very soon. It's a critical piece of your work. It can also be a way for you to study for MCHE 513, since `SymPy` can use both Lagrange's and Kane's methods to generate equations of motion. You can look on [my MCHE513 page](http://www.ucs.louisiana.edu/~jev9637/MCHE513.html) and the [CRAWLAB Code Snippets repository](https://github.com/DocVaughan/CRAWLAB-Code-Snippets/tree/master/SymPy) for examples.

> ***AA*** I will look at those examples as well as the other you sent before. They are on my check-list and have not been checked off or forgotten.

1. Re-write some of the RL algorithms I think I might be using to do some work
    * This is also still on my list, as again I think it will be beneficial

## Current Schedule Outlook
* 11/06 - Crawfish Peeling Team Check-In
* 11/09 - Crawfish Peeling Meeting
* 11/11 - Crawfish Bi-Weekly Report Due
* 11/12 - Running/Jumping Meeting
* 11/13 - Crawfish Peeling Team Check-In
* 11/16 - Crawfish Peeling Meeting
* 11/17 - MCHE 513 Project Presentation Day
* 11/18 - Next Report Due

# Long-term planning

## Upcoming Paper Deadlines
Spring conference season. Thinking about trying to have some work done towards the RL side of things. 

> ***JV*** We definitely need to pick up the pace on your research if we're going to hit a spring conference or two. I think a viable path is to use our existing OpenAI gym environment for the single leg jumping set up to train an agent, then test it on the hardware. The details of the training (we'll tailor the agent, etc to the system) and the experimental results would make for a decent contribution.

> ***AA*** Okay this sounds like a viable path to me as well. This is the direction I am looking to start in with this work. I was not aware there was an OpenAI environment made for this system? Is it a flexible system?

Taking on some work with the Running/Jumping Robot project will also hopefully help me build some paper worthy work with more undergraduate students. I would like to have a discussion with you on this in the near future as to how I should go about starting this process.

> ***JV*** What questions do you have?

> ***AA*** I would say in general, how would I start writing a conference paper? How do I get undergrads to assist in a meaningful way?


## Administrative Deadlines
### Crawfish Peeler
* No immediate deadlines. However, we are thinking forward knowing the next report for the Board will be the first quarter of next year.

> ***JV*** Again, there is *much* less time than you guys think, I think.

> ***AA*** Yes sir.


### Research
* ***Deadline*** to turn in completed, edited thesis to graduate school
  - Two weeks before ***Deadline***: Defended thesis to graduate school for edits
  - Two weeks before ***Defending thesis to the graduate school***: Defend thesis in front of committee
  - Two weeks before ***Defending thesis to the committee***: Submit completed thesis to committee
  - One month before ***Submiting thesis to committee***: submit completed thesis to Dr. Vaughan to edit and make additions as necessary
  * Ideally have the "Mostly Completed" thesis finished 2.5 months before it is to be submitted to the graduate school