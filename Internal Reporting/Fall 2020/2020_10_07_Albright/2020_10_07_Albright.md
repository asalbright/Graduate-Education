Date: 10/07/2020    
Author: Andrew Albright    
Email: andrew.albright1@louisiana.edu    


# Discussion of Key Findings
Per our discussions I have been trying to utilize Spinning Up and the work that they provide as a good way to get me jumped into the topic of RL. I have not been able to make any progress other than running the "Check if your download worked" part of the Spinning Up stuff as well as running through some Jupyter notebooks using Paperspace's/Fast.ai's course. I have found that the latter are not particularly related to RL, more just AI in general.

# Current Difficulties

## Theoretical/Analytical Difficulties
### Understanding the Math 
I am still working through understanding a good bit of the math when reading through the RL papers.

> ***JV*** What parts are giving you trouble, specifically?

> ***AA*** The equations presented in the papers and the technical jargon used to describe the variables used. Putting it together in my head and trying to understand what they are saying is confusing. See random example: of an equation that makes only some sense to me.

## Technical/Implementation Difficulties
### Using Spinning Up/Paperspace/Google Colab
I have found some difficulty getting things up and running while trying to get some hands on work done these past two weeks. I need to reach out to Gerald perhaps or you and figure out the best way to go about figuring out some of these (probably simple errors). 

> ***JV*** Please don't hesitate to ask questions. We want to save our "struggle budget" for the difficult research problems, not getting software set up, etc.

I have tried to utilize Spinning Up and what they offer as far as "tutorials" but cant figure out how to run the examples. I am thinking something went wrong during my downloading of everything but I am not sure. I keep getting an error when trying to import basically anything into the files. 

> ***JV*** Exactly what error do you get?

> ***AA*** That the "modules cannot be found" or something of the sort. There is an issue with the importing of modules such as the 

I assume these modules are included in the Spinning Up download process I just don't know.

> ***JV*** I assumed you followed [their installation instructions](https://spinningup.openai.com/en/latest/user/installation.html)?

> ***AA*** I did, the ones in that link yes. It wasn't very long, and again the "Check Your Install" worked as expected.


# Team Activities
Brennon, Darcy and myself are still moving forward with the designs and concepts of the different sub-systems of the Crawfish Peeler. We are going to be working on having Animations and a presentation for the Board meeting on 10/14/2020

1. Brennon - is still working on the clamping mechanism 
    * Has made some of the modifications to the subsystem
    * Current parts for the system have been printed and cut for testing
2. Darcy - has presented a "loose" concept for the design for the cutting sub-system which will interface with Brennon and I's designs.
    * Still in the process of designing this subsystem
    * I will be sitting down with him to discuss some ideas to get the ball rolling
3. Myself - has completed the designs for the first prototype of the Meat Extractor subsystem.
    * Material has been ordered and cut, awaiting other subsystems material to begin assembly
    
# Papers
## High Level Reviews
1. Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., & De Frcitas, N. (2016). Dueling Network Architectures for Deep Reinforcement Learning. In 33rd International Conference on Machine Learning, ICML 2016 (Vol. 4, pp. 2939–2947). International Machine Learning Society (IMLS).
2. Van Hasselt, H., Guez, A., & Silver, D. (2016). Deep reinforcement learning with double Q-Learning. In 30th AAAI Conference on Artificial Intelligence, AAAI 2016 (pp. 2094–2100). AAAI press.
3. Raibert, M. H., & Craig, J. J. (1981). Hybrid position/force control of manipulators. Journal of Dynamic Systems, Measurement and Control, Transactions of the ASME, 103(2), 126–133. https://doi.org/10.1115/1.3139652
4. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014). Deterministic policy gradient algorithms. In 31st International Conference on Machine Learning, ICML 2014 (Vol. 1, pp. 605–619). International Machine Learning Society (IMLS).
5. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … Wierstra, D. (2016). Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings. International Conference on Learning Representations, ICLR.
6. Liu, Y., Gupta, A., Abbeel, P., & Levine, S. (2018). Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation. In Proceedings - IEEE International Conference on Robotics and Automation (pp. 1118–1125). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/ICRA.2018.8462901
7. Schulman, J., Levine, S., Moritz, P., Jordan, M., & Abbeel, P. (2015). Trust region policy optimization. In 32nd International Conference on Machine Learning, ICML 2015 (Vol. 3, pp. 1889–1897). International Machine Learning Society (IMLS).
8. Gay, G. R., Salomoni, P., & Mirri, S. (2007). E-Learning. In Encyclopedia of Internet Technologies and Applications (pp. 179–184). IGI Global. https://doi.org/10.4018/978-1-59140-993-9.ch026
9. Heess, N., TB, D., Sriram, S., Lemmon, J., Merel, J., Wayne, G., … Silver, D. (2017). Emergence of Locomotion Behaviours in Rich Environments. Retrieved from http://arxiv.org/abs/1707.02286
10. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. Retrieved from http://arxiv.org/abs/1707.06347
11. Mahmood, A. R., Korenkevych, D., Vasan, G., Ma, W., & Bergstra, J. (2018). Benchmarking Reinforcement Learning Algorithms on Real-World Robots. Retrieved from http://arxiv.org/abs/1809.07731
12. Chien-Chern Cheah and Danwei Wang, "Learning impedance control for robotic manipulators," in IEEE Transactions on Robotics and Automation, vol. 14, no. 3, pp. 452-465, June 1998, doi: 10.1109/70.678454.
13. Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2016). Prioritized Experience Replay. CoRR, abs/1511.05952.
14. Racanière, Sébastien et al. “Imagination-Augmented Agents for Deep Reinforcement Learning.” NIPS (2017).
15. Hausknecht, Matthew J. and P. Stone. “Deep Recurrent Q-Learning for Partially Observable MDPs.” AAAI Fall Symposia (2015).
16. Mnih, V. et al. “Playing Atari with Deep Reinforcement Learning.” ArXiv abs/1312.5602 (2013): n. pag.

Many of these I have read in full as I am trying to get into RL more. I do not understand them well enough however to move them to fully read.

> ***JV*** Much better job on this reading. Keep it up.

## Detailed Reviews

### Gay, G. R., Salomoni, P., & Mirri, S. (2007). E-Learning. In Encyclopedia of Internet Technologies and Applications (pp. 179–184). IGI Global. https://doi.org/10.4018/978-1-59140-993-9.ch026

### Summary
"Foundational paper" on the theory behind Q-Learning. Papers describes the theory and how it does in fact converge. My understanding of this proof is that the algorithm can in theory eventually find a solution to a problem provided. The math here, for me is a bit shady still.

### Main Contribution to Relationship Field
Development of an algorithm which other algorithms are built from in the field of AI (ML/ML).

### Relevance to Current Research
Gives me an understanding of part of the base of which some of the more modern algorithms are built from. Specifically ones which I will be looking to better understand which are relevant and commonly used in robotics research.


## Liu, Y., Gupta, A., Abbeel, P., & Levine, S. (2018). Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation. In Proceedings - IEEE International Conference on Robotics and Automation (pp. 1118–1125). Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/ICRA.2018.8462901

### Summary
Develop a system which can translate video input of a person doing a task, into useable training data for an agent to learn from. Special development in this area of study is developing a system which can translate the context of the actions provided to the context of the robot repeating the action. Utilizes RL (TRPO algorithm for virtual training) to train an agent on how to mimic the task using the data created.

### Main Contribution to Relationship Field
Contribution to the field of robotics in so far as its studying the developments of training robots to accomplish tasks through "showing" them. The ideas presented here aim to mimic the way humans learn to do things.

### Relevance to Current Research
I found this paper in looking at different papers these past two weeks and I though it was interesting. I would love to learn more about this topic, but for now it is relevant in that it discusses using RL to train a robot to do a task using data provided.

## Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … Wierstra, D. (2016). Continuous control with deep reinforcement learning. In 4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings. International Conference on Learning Representations, ICLR.

### Summary
Develops the model-free, off-policy actor critic DDPG algorithm which is commonly used in robotics today. Based off of ideas seen in both Deep Q-Learning and DPG algorithms. Trains agents from low-dimensional inputs (joint angles, ets) and raw pixel input form a camera. Able to solve problems in fewer steps as compared to DQN (an algorithm who's ideas were used to build this one). Possible, that if given the time, could solve more difficult problems.

### Main Contribution to Relationship Field
Development of the DDPG algorithm, one which is popular in the robotics scene. Utilized ideas from Q-Learning & DQN, as well as DPG to further develop an algorithm which "*hybridizes*" the ideas.

### Relevance to Current Research
This algorithm is one that I will be looking to do more reading on as it could be something that I choose to use in developing my contribution to the field. 

## Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., & Riedmiller, M. (2014). Deterministic policy gradient algorithms. In 31st International Conference on Machine Learning, ICML 2014 (Vol. 1, pp. 605–619). International Machine Learning Society (IMLS).

### Summary
Development of a deterministic policy gradient (DPG), off-policy actor-critic algorithm. One which is used as the base for other algorithms such as DDPG. This algorithm is analogous to the algorithms of the stochastic policy gradient type.

### Main Contribution to Relationship Field
Development of one of the foundational algorithms used in RL. 

### Relevance to Current Research
I read this paper after having skimmed through the paper on DDPG. I wanted to establish some sort of understanding of DPG before reading through DDPG which builds on some of the ideas of DPG.

## Mahmood, A. R., Korenkevych, D., Vasan, G., Ma, W., & Bergstra, J. (2018). Benchmarking Reinforcement Learning Algorithms on Real-World Robots. Retrieved from http://arxiv.org/abs/1809.07731

### Summary
Paper on deploying RL algorithms to actual robots. Algorithms chosen to deploy were *trust region policy optimization* (TRPO), *proximal policy optimization* (PPO), *soft Q-learning* (Soft-Q) and *deep deterministic policy gradient* (DDPG). Goal is to analyze algorithm performance across different tasks as well as track characteristics such as hyperparameter and network initialization. 

### Main Contribution to Relationship Field
Shows results of deploying a select few algorithms on physical hardware as opposed to in simulation. Claim that, for their tests, the choice of hyperparameters was more important that the choice of algorithm. Showed, that in general, scripted solutions tended to be better than RL solutions, though in the case when scripting a solution is challenging, RL might have the edge.

### Relevance to Current Research
This paper is useful for me as it illustrates some examples of RL being deployed on actual hardware. This is pretty cool and exciting to me, though I think my next steps are simulation before I jump on actual hardware. I think as well, this paper helps narrow down my search for knowledge in the field of RL, pointing me towards algorithms I might look more into.

> ***JV*** Much better job on this reading. Keep it up.

Per our discussion I have organized these in the deep dive section of my papers organization. However I plan to return to them in time to better understand them and attempt to replicate some of what has been done.

> ***JV*** Going back to papers is something that we will regularly do. You'll often see that you not only understand the individual paper better, but are able to better see its connection to other work.

## Future Reading
### Types of papers I plan to read in the next two weeks:
1. I will be looking into reading more about the specific algorithms that are used in robotics. Essentially, if I am understanding some of the material I am reading, ones which excel at learning in continuous action spaces.

### What I am aiming to achieve from reading these papers: 
1. Grow, and more importantly guide my understanding towards ideas and developments that are relevant to what I want to learn about and contribute to.

### Why I am looking to achieve that:
1. So that I can start getting addicted to the work I am doing here. Reading papers about "foundational" knowledge is important but not particularly fun, I want to start following a path that leads to me doing testing on things I am directly interested in.

# Plan for the Next Two Weeks
At a glance, have a presentation ready for the Crawfish Peeler Board meeting next week (including animations), get started in utilizing Paperspace or Google Colab after discussing with Dr. Vaughan and Gerald, and continue reading into topics more directly related to what I am interested in.

## Ideas Looking Forward

### Crawfish Peeler Project
As we continue our designs we are currently waiting for some material to arrive so we can begin testing. I have some ideas to share with Darcy related to the Tail Cutting Subsystem which might make the design simpler and easier from a mechanical and controls viewpoint.

> ***JV*** Sounds good. 

### Robotics Research
Since my introduction to RL (new topic to me since I arrived here) my views in what I want to do have changed since I started. I've become more interested in the topic of RL, and the more I read, the more I want to incorporate it into my contribution to the field. I want to get some things into simulation so I can start realizing the potential that exists and would like your help getting there. I think my steps are first figuring out how I can get an environment set up where I can run the programs I need to run.

## Current Next Steps

### Crawfish Peeler Project
Have a presentation prepared for the board meeting on 10/14/2020. Have animations in particular to be put into the presentation.

### Robotics Research
Getting either Paperspace or Google Colab figured out so I can start running and writing code effectively. Perferably Paperspace as they support Jupyter Notebooks which are new to me still but seem expremely effective. Googles interface seems very similar, too I suppose. I just need to figure one out.

## Expectations for Next Report (10/21/2020)
1. Worked through the tutorial examples in Spinning Up in RL and synced that code to GitHub.
    * Just need to figure out why the modules are not being recognized

2. Run something in Google Colab or Paperspace.
    * Need to discuss more in depth with Gerald and you on how I can get set up now so I am not doing it later. I would perfer to start using these environments now instead of my old Linux machine so I am used to it

3. Learn how to model a system utilizing Python
    * One of the undergrads asked for some help with this and I realized I've never used Python to model any kind of system
    
> ***JV*** We have a lot of examples of this. Some of our work uses the older [`odeint` method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html) in SciPy. However, it is not recommended that you use [`solve_ivp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html), so we've switched to it for all new work. For some examples, ou can look at:
> * https://github.com/DocVaughan/CRAWLAB-Code-Snippets/tree/master/Simple%20Simulations
> * https://github.com/DocVaughan/MCHE485---Mechanical-Vibrations/tree/Spring2020
> * https://github.com/DocVaughan/MCHE513---Intermediate-Dynamics

> ***AA*** I will add theses links to my to-do list.

1. Re-write some of the RL algorithms I think I might be using to do some work
    * this might be a streatch, I am not sure if my Python skills are quite there, but I'll give it a try. I think it could help my understanding of what is happening.
    
> ***JV*** You can find a few implementations intended for teaching/learning that are written in "pure" NumPy/SciPy rather than a machine learning library. A search for "Machine Learning from Scratch" or "Reinforcement Learning from scratch", possibly adding "NumPy" to either search query should turn them up. If you can't find them, please let me know. I have the links somewhere. [This one](https://github.com/ddbourgin/numpy-ml) that I found with that search looks fairly well done.

> ***AA*** I will add this to my to-do list as well.


1. Work on writing this throughout the two weeks instead of on the last two days
    * Both because writing this in a rushed sense is not great and becuase it will force me to reach out and ask questions when I run into issues earlier.
    
> ***JV*** That is definitely the best strategy, I think. Use this as a daily notebook/log, then just clean it up on submission day.

> ***AA*** Sounds like a plan.

## Current Schedule Outlook
* 10/08 - Crawfish Peeling Team Check-In
* 10/12 - Crawfish Peeling Meeting
* 10/14 - Crawfish Board Meeting
* 10/14 - Crawfish Bi-Weekly Report Due
* 10/15 - Crawfish Peeling Team Check-In
* 10/16 - Full Team Meeting
* 10/19 - Crawfish Peeling Meeting
* 10/21 - Next Report Due

# Long-term planning

## Upcoming Paper Deadlines
Spring conference season. Thinking about trying to have some work done towards the RL side of things. 

## Administrative Deadlines
### Crawfish Peeler
* Board Meeting on 10/14
  * We are going to be developing a presentation for this event.

### Research
* ***Deadline*** to turn in completed, edited thesis to graduate school
  - Two weeks before ***Deadline***: Defended thesis to graduate school for edits
  - Two weeks before ***Defending thesis to the graduate school***: Defend thesis in front of committee
  - Two weeks before ***Defending thesis to the committee***: Submit completed thesis to committee
  - One month before ***Submiting thesis to committee***: submit completed thesis to Dr. Vaughan to edit and make additions as necessary
  * Ideally have the "Mostly Completed" thesis finished 2.5 months before it is to be submitted to the graduate school